{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SampleRenderDataset at 0x7f7d661ebbb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Grayscale,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    RandomHorizontalFlip,\n",
    "    ToTensor,\n",
    "    ToPILImage,\n",
    ")\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.structures import Meshes\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from src.utilities.util import scale_geometry\n",
    "from src.augment.geoaug import GeoAugment\n",
    "\n",
    "def pyramid_transform(img_size, mean=0, std=1):\n",
    "    transform = Compose([            \n",
    "        Resize([img_size, img_size]),\n",
    "        RandomHorizontalFlip(),\n",
    "        Grayscale(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(mean), std=(std)),\n",
    "    ])\n",
    "    def final_transform(img):        \n",
    "        return transform(img)\n",
    "    \n",
    "    return final_transform\n",
    "\n",
    "class SampleRenderDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        self.image_root = config.data_renders_dir        \n",
    "        self.image_mean = config.fast_image_mean\n",
    "        self.image_std = config.fast_image_std\n",
    "        self.image_size = config.fast_image_size\n",
    "                \n",
    "        self.transform = pyramid_transform(self.image_size, \n",
    "                                           self.image_mean, self.image_std)\n",
    "        self.img_ds = ImageFolder(self.image_root, transform=self.transform)        \n",
    "        \n",
    "    def scale(self, t, size):\n",
    "        return F.interpolate(t[None], size=size, mode='bilinear', align_corners=True)[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_ds)\n",
    "    \n",
    "    def get_samples(self, idx):\n",
    "        f =  img_ds.imgs[idx][0]\n",
    "        f = f.replace('renders', 'samples').replace('.png', '.pth')\n",
    "        return torch.load(f)\n",
    "   \n",
    "    def __getitem__(self, idx):              \n",
    "        res = {}\n",
    "        image, label = self.img_ds[idx]\n",
    "        res['image'] =  image\n",
    "        res['label'] =  label\n",
    "        res['samples'] =  self.get_samples(idx)        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "from src.config import get_parser\n",
    "\n",
    "config = get_parser().parse_args(args=[])   \n",
    "\n",
    "config.data_renders_dir = '/home/bobi/Desktop/pic2mesh/data/augmented/renders'\n",
    "\n",
    "ds = SampleRenderDataset(config)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]),\n",
       " 'label': 0,\n",
       " 'samples': tensor([[ 0.0339, -0.2852,  0.4394],\n",
       "         [ 0.2332,  0.4915,  0.5623],\n",
       "         [ 0.4892, -0.3453,  0.2136],\n",
       "         ...,\n",
       "         [-0.2259, -0.1352,  0.1179],\n",
       "         [ 0.0136,  0.2328,  0.4754],\n",
       "         [-0.3026,  0.8602,  0.1647]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 18600\n",
       "    Root location: /home/bobi/Desktop/pic2mesh/data/augmented/renders"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_root = '/home/bobi/Desktop/pic2mesh/data/augmented/renders'\n",
    "img_ds = ImageFolder(image_root)\n",
    "img_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=512x512 at 0x7F7D671CB9A0>, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "path = img_ds.imgs[idx][0].replace('renders', 'samples').replace('.png', '.pth')\n",
    "torch.load(path).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.data.sample_render.SampleRenderDataModule at 0x7f02b46c6640>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.sample_render import SampleRenderDataModule\n",
    "\n",
    "from src.config import get_parser\n",
    "\n",
    "config = get_parser().parse_args(args=[])   \n",
    "\n",
    "config.data_renders_dir = '/home/bobi/Desktop/pic2mesh/data/augmented/renders'\n",
    "\n",
    "dm = SampleRenderDataModule(config)\n",
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]]]]),\n",
       " 'label': tensor([183,  71,  51,  23,  51,  31,  72,  18]),\n",
       " 'samples': tensor([[[-0.1250,  0.7768,  0.2303],\n",
       "          [-0.3375,  0.3318,  0.8441],\n",
       "          [-0.2035,  0.5366,  0.7530],\n",
       "          ...,\n",
       "          [-0.1673, -0.4104,  0.8658],\n",
       "          [ 0.0659,  0.5133,  0.7123],\n",
       "          [ 0.2554,  0.2266,  0.7179]],\n",
       " \n",
       "         [[ 0.2257, -0.5672,  0.6166],\n",
       "          [-0.1178,  0.2712,  0.5219],\n",
       "          [ 0.4087, -0.6979,  0.0245],\n",
       "          ...,\n",
       "          [ 0.5879,  0.1286,  0.4711],\n",
       "          [ 0.3634,  0.2875,  0.5589],\n",
       "          [ 0.6670,  0.2124,  0.3909]],\n",
       " \n",
       "         [[-0.0575, -0.0218,  0.5929],\n",
       "          [ 0.7268,  0.7394,  0.2878],\n",
       "          [ 0.7242, -0.0922,  0.1462],\n",
       "          ...,\n",
       "          [-0.2660,  0.8086,  0.1605],\n",
       "          [ 0.3365, -0.4463,  0.5536],\n",
       "          [ 0.5518,  1.1550,  0.1387]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.6065,  0.0738,  0.5791],\n",
       "          [ 0.2575,  0.3319,  1.0713],\n",
       "          [ 0.1805, -0.1128,  1.0706],\n",
       "          ...,\n",
       "          [-0.4405, -0.4787,  0.8096],\n",
       "          [ 0.8020, -0.3686,  0.2612],\n",
       "          [-0.2444, -0.8760,  0.9672]],\n",
       " \n",
       "         [[-0.0936, -0.2476,  0.7199],\n",
       "          [-0.3763, -0.7013, -0.0021],\n",
       "          [-0.5064, -0.4357,  0.0797],\n",
       "          ...,\n",
       "          [ 0.1466, -0.1446,  0.4515],\n",
       "          [ 0.2147,  0.7454,  0.3856],\n",
       "          [ 0.0938,  0.5407,  0.5398]],\n",
       " \n",
       "         [[ 0.0634,  1.4338,  0.2255],\n",
       "          [ 0.3524,  0.1214,  0.2946],\n",
       "          [-0.3287,  1.0210,  0.5248],\n",
       "          ...,\n",
       "          [-0.3664,  0.4374,  0.5809],\n",
       "          [-0.5808, -0.0784,  0.2572],\n",
       "          [-0.5861, -0.0612,  0.2377]]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = next(iter(dm.train_dataloader()))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_05",
   "language": "python",
   "name": "pytorch3d_05"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
