{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from src.config import get_parser\n",
    "#from src.models.gan import GAN\n",
    "from src.callback.points_image import PointsImage\n",
    "from src.data.masked_datamodule import MaskedDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAN(\n",
       "  (G): Generator(\n",
       "    (head): ConvBlock(\n",
       "      (conv): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (body): Sequential(\n",
       "      (b1): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (b2): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (b3): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (tail): Sequential(\n",
       "      (0): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (D): Discriminator(\n",
       "    (model): Sequential(\n",
       "      (b0): ConvBlock(\n",
       "        (conv): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (b1): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (b2): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (b3): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (linear): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (R): MeshPointsRenderer()\n",
       "  (edge_loss): EdgeLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.loss import ( \n",
    "    mesh_edge_loss, \n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "\n",
    "from src.models.discriminator import Discriminator\n",
    "from src.models.generator import Generator\n",
    "from src.render.mesh_renderer import MeshPointsRenderer\n",
    "from src.loss.edge_loss import EdgeLoss\n",
    "from src.util import grid_to_list\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.automatic_optimization = False\n",
    "        self.mean = sum(hparams.image_mean) / len(hparams.image_mean)\n",
    "        self.std = sum(hparams.image_std) / len(hparams.image_std)\n",
    "        \n",
    "        self.G = Generator(hparams)\n",
    "        self.D = Discriminator(hparams)        \n",
    "        # Renderer requires device, created in .to() step\n",
    "        self.R = MeshPointsRenderer(hparams)        \n",
    "        self.edge_loss = EdgeLoss(hparams)\n",
    "        \n",
    "     \n",
    "    def forward(self, shape, style):\n",
    "        return self.G(shape, style)\n",
    "    \n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.mse_loss(y_hat, y)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):        \n",
    "        img_patch = batch['img_patch']\n",
    "        points = batch['points']        \n",
    "        pt_normals = batch['normals']\n",
    "        faces = batch['faces']\n",
    "        bs = img_patch.size(0)\n",
    "        pt_normals = pt_normals.reshape(bs, 3, -1).permute(0, 2, 1)        \n",
    "        self.R.setup(points.device)        \n",
    "        \n",
    "        # train generator\n",
    "        if optimizer_idx == 0:            \n",
    "            vertices = points + torch.randn_like(points) * self.hparams.G_noise_amp            \n",
    "            vertices = self.G(vertices)            \n",
    "            renders =  self.R(vertices).permute(0, 3, 1, 2)            \n",
    "            renders =  renders[ :, :3, :, :]            \n",
    "            renders = (renders - self.mean) / self.std                        \n",
    "            # ground truth result (ie: all fake)\n",
    "            # put on GPU because we created this tensor inside training_loop\n",
    "            valid = torch.ones(bs, 1).type_as(points)\n",
    "            \n",
    "            render_loss = self.adversarial_loss(self.D(renders), valid)\n",
    "            \n",
    "            #cos_sim = torch.cosine_similarity(normals, pt_normals, dim=-1)\n",
    "            #normal_consistency_loss = -(cos_sim.sum() / cos_sim.numel() - 1.)\n",
    "            \n",
    "            edge_loss = self.edge_loss(vertices)            \n",
    "            g_loss = render_loss + edge_loss * self.hparams.mesh_edge_loss_weight \\\n",
    "                + edge_loss * self.hparams.mesh_edge_loss_weight\n",
    "            #+ normal_consistency_loss * self.hparams.mesh_normal_consistency_weight\n",
    "            #tqdm_dict = {'g_loss': g_loss}\n",
    "            \n",
    "            self.log(\"loss/g_loss\", g_loss, on_epoch=True, prog_bar=True)\n",
    "            self.log(\"loss/render_loss\", render_loss, on_epoch=True)\n",
    "            self.log(\"loss/edge_loss\", edge_loss, on_epoch=True)\n",
    "            #self.log(\"loss/normal_consistency_loss\", normal_consistency_loss, on_epoch=True)\n",
    "            return OrderedDict({ 'loss': g_loss, })\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:            \n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "            # how well can it label as real?\n",
    "            valid = torch.ones(bs, 1).type_as(points)         \n",
    "\n",
    "            real_loss = self.adversarial_loss(self.D(imgs), valid)\n",
    "\n",
    "            # how well can it label as fake?\n",
    "            fake = torch.zeros(imgs.size(0), 1).type_as(points)            \n",
    "                        \n",
    "            vertices = self.G(points)            \n",
    "            renders =  self.R(vertices).permute(0, 3, 1, 2)\n",
    "            renders =  renders[ :, :3, :, :]\n",
    "            renders = (renders - self.mean) / self.std            \n",
    "            fake_loss = self.adversarial_loss(\n",
    "                self.D(renders.detach()), fake)\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            tqdm_dict = {'d_loss': d_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': d_loss,                \n",
    "            })\n",
    "            self.log(\"loss/d_loss\", d_loss, on_step=True, on_epoch=True, prog_bar=True)            \n",
    "            return output\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr_g = self.hparams.lr_g\n",
    "        lr_d = self.hparams.lr_d\n",
    "        b1 = self.hparams.beta1\n",
    "        b2 = self.hparams.beta2      \n",
    "        opt_g = torch.optim.Adam(self.G.parameters(), \n",
    "                                 lr=lr_g, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.D.parameters(), \n",
    "                                 lr=lr_d, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "    \n",
    "config = get_parser().parse_args(args=[])\n",
    "#config.batch_size = 8\n",
    "model = GAN(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.data.masked_datamodule.MaskedDataModule at 0x7f5ff31d89a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = MaskedDataModule(config)\n",
    "dm.setup()\n",
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | G         | Generator          | 1.8 M \n",
      "1 | D         | Discriminator      | 1.8 M \n",
      "2 | R         | MeshPointsRenderer | 0     \n",
      "3 | edge_loss | EdgeLoss           | 0     \n",
      "-------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.264    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c9902958ba41faa0d5ca4c1358998c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=5, progress_bar_refresh_rate=20, \n",
    "                     callbacks=[\n",
    "                         #LogExportCallback(config),\n",
    "                         PointsImage(config)\n",
    "                         ])\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pulsar(\n",
       "  (renderer): PulsarPointsRenderer(\n",
       "    (rasterizer): PointsRasterizer(\n",
       "      (cameras): FoVPerspectiveCameras()\n",
       "    )\n",
       "    (renderer): Renderer(width=128, height=128, max_num_balls=1000000)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 300/951 [05:12<11:18, 1.04s/it, v_num=1, loss/g_loss_step=0.287]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.mesh_normal_consistency_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = torch.cosine_similarity(torch.rand([2, 65536, 3]), torch.rand([2, 65536, 3]), dim=-1)\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_0.3",
   "language": "python",
   "name": "pytorch3d_0.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
