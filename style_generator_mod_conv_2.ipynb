{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (b1): GenBlock(\n",
      "    (mod_conv): ModulateConvBlock(\n",
      "      (style): DenseBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (activate): Identity()\n",
      "      )\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (to_points): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "  )\n",
      "  (b2): GenBlock(\n",
      "    (mod_conv): ModulateConvBlock(\n",
      "      (style): DenseBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (activate): Identity()\n",
      "      )\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (to_points): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (b3): GenBlock(\n",
      "    (mod_conv): ModulateConvBlock(\n",
      "      (style): DenseBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (activate): Identity()\n",
      "      )\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (to_points): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 64, 64])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.models.layers import ModulateConvBlock\n",
    "#from src.models.blocks import ConvBlock\n",
    "from src.utilities.util import grid_to_list\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, in_channel, out_ch, ker_size, stride, padding):\n",
    "        super(ConvBlock,self).__init__()\n",
    "        self.add_module('conv',nn.Conv2d(in_channel, out_ch, \n",
    "                                         kernel_size=ker_size,\n",
    "                                         stride=stride,\n",
    "                                         padding=padding)),\n",
    "        self.add_module('norm',nn.BatchNorm2d(out_ch)),\n",
    "        #self.add_module('norm',nn.InstanceNorm2d(out_ch)),\n",
    "        self.add_module('LeakyRelu',nn.LeakyReLU(0.2, inplace=True))\n",
    "        #self.add_module('Nonlinearity',nn.Tanh())\n",
    "        #self.add_module('GELU',nn.GELU())\n",
    "        #self.add_module('Nonlinearity', nn.Hardswish(inplace=True))\n",
    "\n",
    "#     def weights_init(m):\n",
    "#         classname = m.__class__.__name__\n",
    "#         if classname.find('Conv2d') != -1:\n",
    "#             m.weight.data.normal_(0.0, 0.02)\n",
    "#         elif classname.find('Norm') != -1:\n",
    "#             m.weight.data.normal_(1.0, 0.02)\n",
    "#             m.bias.data.fill_(0) \n",
    "\n",
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, latent_size, in_ch, out_ch, kernel, stride, padding, pool=None):\n",
    "        super(GenBlock, self).__init__()        \n",
    "        self.mod_conv =  ModulateConvBlock(latent_size, in_ch, out_ch, kernel)\n",
    "        #self.mod_conv = ConvBlock(in_ch, out_ch, kernel, stride=stride, padding=padding)\n",
    "        self.conv = ConvBlock(out_ch, out_ch, kernel, stride=stride, padding=padding)\n",
    "        self.to_points = nn.Conv2d(out_ch, 3, kernel, stride=stride, padding=padding)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=pool, stride=pool) if pool else None\n",
    "    \n",
    "    def upscale(self, x, scale_factor):\n",
    "        return F.interpolate(x, scale_factor=scale_factor, mode='bilinear', \n",
    "                             align_corners=True)# if scale_factor else x     \n",
    "    \n",
    "    def forward(self, x, z, prev_vrt):        \n",
    "        y = self.mod_conv(x, z)\n",
    "        y = self.conv(y)\n",
    "        vrt = self.to_points(self.pool(y)) if self.pool else self.to_points(y)                    \n",
    "        scale_factor = prev_vrt.size(-1) // vrt.size(-1)        \n",
    "        if scale_factor > 1:\n",
    "            vrt = self.upscale(vrt, scale_factor)\n",
    "        vrt = vrt + prev_vrt\n",
    "        return x, vrt\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Generator, self).__init__()                      \n",
    "        latent_size, in_ch, out_ch, ker_size, stride, padding= (opt.latent_size,\n",
    "            opt.G_in_ch, opt.G_out_ch, opt.ker_size, opt.stride, opt.padd_size)\n",
    "        self.head =  ConvBlock(in_ch, out_ch, ker_size, stride=stride, padding=padding)\n",
    "        self.pools = [4, 2, None]\n",
    "        \n",
    "        self.b1 = GenBlock(latent_size, out_ch, out_ch, ker_size, stride=stride, \n",
    "                      padding=padding, pool=self.pools[0])\n",
    "        self.b2 = GenBlock(latent_size, out_ch, out_ch, ker_size, stride=stride, \n",
    "                      padding=padding, pool=self.pools[1])\n",
    "        self.b3 = GenBlock(latent_size, out_ch, out_ch, ker_size, stride=stride, \n",
    "                           padding=padding, pool=self.pools[2])\n",
    "    \n",
    "    def forward(self, points, latents):                        \n",
    "        x = self.head(points)\n",
    "        \n",
    "        x, vrt = self.b1(x, latents[0], points)\n",
    "        x, vrt = self.b2(x, latents[1], vrt)\n",
    "        x, vrt = self.b3(x, latents[2], vrt)\n",
    "        \n",
    "        #vrt = grid_to_list(vrt)\n",
    "        return vrt\n",
    " \n",
    "\n",
    "from src.config import get_parser\n",
    "\n",
    "config = get_parser().parse_args(args=[])\n",
    "G = Generator(config)\n",
    "print(G)\n",
    "\n",
    "G(torch.rand(5, 3, 64, 64), [torch.rand(5, config.latent_size) for _ in range(3)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device\n",
    "\n",
    "G = G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.data.masked_datamodule.MaskedDataModule at 0x7f0b9c0b0760>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.masked_datamodule import MaskedDataModule\n",
    "\n",
    "\n",
    "config.data_patch_size = 16\n",
    "config.batch_size = 8\n",
    "config.num_workers = 6\n",
    "dm = MaskedDataModule(config)\n",
    "dm.setup()\n",
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(G.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 16, 16])\n",
      "0 torch.Size([8, 3, 16, 16])\n",
      "0 tensor(0.4121, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "1 torch.Size([8, 3, 16, 16])\n",
      "1 tensor(4.7421, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "2 torch.Size([8, 3, 16, 16])\n",
      "2 tensor(3.7910, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "3 torch.Size([8, 3, 16, 16])\n",
      "3 tensor(2.0477, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "4 torch.Size([8, 3, 16, 16])\n",
      "4 tensor(1.2277, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "5 torch.Size([8, 3, 16, 16])\n",
      "5 tensor(1.3469, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "6 torch.Size([8, 3, 16, 16])\n",
      "6 tensor(1.7351, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "7 torch.Size([8, 3, 16, 16])\n",
      "7 tensor(1.6608, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "8 torch.Size([8, 3, 16, 16])\n",
      "8 tensor(1.1330, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "9 torch.Size([8, 3, 16, 16])\n",
      "9 tensor(1.1097, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "10 torch.Size([8, 3, 16, 16])\n",
      "10 tensor(1.0508, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "11 torch.Size([8, 3, 16, 16])\n",
      "11 tensor(1.0917, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "12 torch.Size([8, 3, 16, 16])\n",
      "12 tensor(0.8316, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "13 torch.Size([8, 3, 16, 16])\n",
      "13 tensor(0.6080, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "14 torch.Size([8, 3, 16, 16])\n",
      "14 tensor(0.5740, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "15 torch.Size([8, 3, 16, 16])\n",
      "15 tensor(0.6304, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "16 torch.Size([8, 3, 16, 16])\n",
      "16 tensor(0.7562, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "17 torch.Size([8, 3, 16, 16])\n",
      "17 tensor(0.5811, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "18 torch.Size([8, 3, 16, 16])\n",
      "18 tensor(0.5473, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "19 torch.Size([8, 3, 16, 16])\n",
      "19 tensor(0.5833, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "20 torch.Size([8, 3, 16, 16])\n",
      "20 tensor(0.6515, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "21 torch.Size([8, 3, 16, 16])\n",
      "21 tensor(0.5415, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "22 torch.Size([8, 3, 16, 16])\n",
      "22 tensor(0.4798, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "23 torch.Size([8, 3, 16, 16])\n",
      "23 tensor(0.4476, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "24 torch.Size([8, 3, 16, 16])\n",
      "24 tensor(0.6303, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "25 torch.Size([8, 3, 16, 16])\n",
      "25 tensor(0.5760, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "26 torch.Size([8, 3, 16, 16])\n",
      "26 tensor(0.5729, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "27 torch.Size([8, 3, 16, 16])\n",
      "27 tensor(0.4847, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "28 torch.Size([8, 3, 16, 16])\n",
      "28 tensor(0.3044, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "29 torch.Size([8, 3, 16, 16])\n",
      "29 tensor(0.4206, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "30 torch.Size([8, 3, 16, 16])\n",
      "30 tensor(0.4290, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "31 torch.Size([8, 3, 16, 16])\n",
      "31 tensor(0.4075, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "32 torch.Size([8, 3, 16, 16])\n",
      "32 tensor(0.4416, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "33 torch.Size([8, 3, 16, 16])\n",
      "33 tensor(0.3535, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "34 torch.Size([8, 3, 16, 16])\n",
      "34 tensor(0.3307, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "35 torch.Size([8, 3, 16, 16])\n",
      "35 tensor(0.2364, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "36 torch.Size([8, 3, 16, 16])\n",
      "36 tensor(0.3412, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "37 torch.Size([8, 3, 16, 16])\n",
      "37 tensor(0.3828, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "38 torch.Size([8, 3, 16, 16])\n",
      "38 tensor(0.3782, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "39 torch.Size([8, 3, 16, 16])\n",
      "39 tensor(0.3204, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "40 torch.Size([8, 3, 16, 16])\n",
      "40 tensor(0.3732, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "41 torch.Size([8, 3, 16, 16])\n",
      "41 tensor(0.3957, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "torch.Size([8, 3, 16, 16])\n",
      "42 torch.Size([8, 3, 16, 16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e72535521607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m            \u001b[0mpoints_fine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints_fine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m            \u001b[0mvrt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints_fine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m            \u001b[0mvrt_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mwalk_stack\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_lineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " for idx, batch in enumerate(iter(dm.train_dataloader())):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        points_coarse = batch['points_coarse'].to(device)#batch['points']             \n",
    "        points_fine = batch['points'].to(device)\n",
    "        bs = points_fine.size(0)        \n",
    "        print(points_fine.shape)\n",
    "        # train generator\n",
    "        with torch.autograd.set_detect_anomaly(True):            \n",
    "            points_noise = points_coarse + torch.randn_like(points_coarse) * config.G_noise_amp\n",
    "            z_size = config.latent_size\n",
    "            zero_styles = [torch.zeros(bs, z_size, device=points_coarse.device) for _ in range(3)]\n",
    "            vertices = G(points_noise, zero_styles)\n",
    "            print(idx, vertices.shape)\n",
    "            #break\n",
    "            #print(vertices.shape)\n",
    "            vrt_loss = F.mse_loss(vertices, points_noise.reshape_as(vertices))\n",
    "            \n",
    "            points_fine = points_fine.reshape_as(vertices)\n",
    "            vrt_loss = F.l1_loss(vertices, points_fine)\n",
    "            \n",
    "            vrt_loss.backward()\n",
    "            optimizer.step()\n",
    "            print(idx, vrt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_0.3",
   "language": "python",
   "name": "pytorch3d_0.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
