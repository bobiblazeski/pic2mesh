{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as TR\n",
    "import pytorch_lightning as pl\n",
    "from src.config import get_parser\n",
    "from src.utilities.util import (\n",
    "    grid_to_list,\n",
    "    list_to_grid,\n",
    "    make_faces,\n",
    ")\n",
    "from src.utilities.alignment import align\n",
    "from src.utilities.vertex_normals import VertexNormals\n",
    "from src.render.mesh_points_renderer import MeshPointsRenderer\n",
    "from types import SimpleNamespace\n",
    "from src.models.normal_generator import Generator\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "def normalize(t):\n",
    "    return TR.Normalize(t.mean(), t.std())(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512]) torch.Size([1, 3, 512, 512]) torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "d = torch.load('./data/npz/ada_wong_resident_evil_256.pth')\n",
    "vertices = d['vertices'][None].to(device)\n",
    "normals = d['normals'][None].to(device)\n",
    "\n",
    "sz = 32\n",
    "sh = 90\n",
    "sv = 64\n",
    "vertices = vertices[:, :, sv:sv+sz, sh:sh+sz]\n",
    "normals = normals[:, :, sv:sv+sz, sh:sh+sz]\n",
    "\n",
    "vertices = F.interpolate(vertices, scale_factor=16)\n",
    "normals =  F.interpolate(normals, scale_factor=16)\n",
    "normals = F.normalize(normals, dim=1)\n",
    "colors = torch.ones_like(vertices)\n",
    "print(vertices.shape, normals.shape, colors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F519DA7BF10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_aligned(vrt, nrm):\n",
    "    p_v, p_n =  align(grid_to_list(vrt), \n",
    "                      grid_to_list(nrm),\n",
    "                      rotate=False)\n",
    "    return list_to_grid(p_v), list_to_grid(p_n)\n",
    "    \n",
    "#224x224\n",
    "config = get_parser().parse_args(args=[])\n",
    "config.fast_image_size = 224\n",
    "config.fast_baseline_size = vertices.size(-1)\n",
    "config.viewpoint_distance = 0.8\n",
    "\n",
    "config.fast_image_size = 224\n",
    "config.fast_baseline_size = vertices.size(-1)\n",
    "config.viewpoint_distance = 1.0\n",
    "\n",
    "R = MeshPointsRenderer(config).to(device)\n",
    "R\n",
    "\n",
    "vrt_a, _ = get_aligned(vertices, normals)\n",
    "\n",
    "renders = R(vrt_a, colors, grayscale=False)\n",
    "rndr_mean, rndr_std =  renders.mean(),  renders.std()\n",
    "print(renders.shape)\n",
    "TR.ToPILImage()(renders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt_nrm = VertexNormals(config, size=vertices.size(-1)).to(device)\n",
    "\n",
    "def calculate_normals(t):\n",
    "    nrm = vrt_nrm.vertex_normals_fast(grid_to_list(t))\n",
    "    return list_to_grid(nrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sn = SimpleNamespace(fast_generator_channels=[ 128, 128, 128, 128])\n",
    "\n",
    "G = Generator(sn).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(G.parameters(), lr=0.0001)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4308, 0.4252, 0.4113]) tensor([0.1197, 0.1178, 0.1165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#img_f = '/home/bobi/Desktop/pic2mesh/data/textures/mouth.jpg'\n",
    "img_f = '/home/bobi/Desktop/pic2mesh/data/textures/Zbrush-Details.jpg'\n",
    "\n",
    "img = Image.open(img_f)\n",
    "img_size = 224\n",
    "transform = TR.Compose([\n",
    "    TR.Resize([img_size, img_size]),\n",
    "    #Grayscale(),\n",
    "    TR.ToTensor(),\n",
    "    #Normalize(mean=(mean), std=(std)),\n",
    "])\n",
    "style_img_t = transform(img)\n",
    "img_mean = style_img_t.reshape(3, -1).mean(dim=1)\n",
    "img_std = style_img_t.reshape(3, -1).std(dim=1)\n",
    "print(img_mean, img_std)\n",
    "transform = TR.Compose([\n",
    "    TR.Resize([img_size, img_size]),    \n",
    "    TR.ToTensor(),\n",
    "    TR.Normalize(mean=img_mean, std=img_std),\n",
    "])\n",
    "style_img_t = transform(img)[None].to(device)\n",
    "style_img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.3332, 0.3332],\n",
       "        [0.3332, 0.3333, 0.3332],\n",
       "        [0.3332, 0.3332, 0.3333]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gram_matrix(t):\n",
    "    a, b, c, d = t.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = t.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "gram_matrix(style_img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "#[0,5,10,19,28] are the index of the layers we will be using to calculate the loss as per the paper of NST\n",
    "#Defining a class that for the model\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG,self).__init__()\n",
    "        #Here we will use the following layers and make an array of their indices\n",
    "        # 0: block1_conv1\n",
    "        # 5: block2_conv1\n",
    "        # 10: block3_conv1\n",
    "        # 19: block4_conv1\n",
    "        # 28: block5_conv1\n",
    "        self.req_features= [\n",
    "            #'0',\n",
    "            '5',\n",
    "            '10',\n",
    "            '19',\n",
    "            #'28'\n",
    "        ] \n",
    "        #Since we need only the 5 layers in the model so we will be dropping all the rest layers from the features of the model\n",
    "        self.model=models.vgg19(pretrained=True).features[:29] #model will contain the first 29 layers\n",
    "    \n",
    "   \n",
    "    #x holds the input tensor(image) that will be feeded to each layer\n",
    "    def forward(self,x):\n",
    "        #initialize an array that wil hold the activations from the chosen layers\n",
    "        features=[]\n",
    "        #Iterate over all the layers of the mode\n",
    "        for layer_num,layer in enumerate(self.model):\n",
    "            #activation of the layer will stored in x\n",
    "            x=layer(x)\n",
    "            #appending the activation of the selected layers and return the feature array\n",
    "            if (str(layer_num) in self.req_features):\n",
    "                features.append(x)\n",
    "                \n",
    "        return features\n",
    "    \n",
    "model =  VGG().to(device).eval() \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 1\n",
    "style_img_t = style_img_t.expand(bs, -1, -1, -1)\n",
    "style_img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def all_gram_matrices(features):\n",
    "    return [gram_matrix(f) for f in features]\n",
    "\n",
    "def calc_style_loss(gen_features, orig_gms):\n",
    "    gen_gms = all_gram_matrices(gen_features)    \n",
    "    return sum([\n",
    "       torch.mean((G-A)**2)\n",
    "       for (G, A) in\n",
    "       zip(gen_gms, orig_gms)\n",
    "    ])\n",
    "\n",
    "orig_features =  model(style_img_t)\n",
    "orig_gms = all_gram_matrices(orig_features)\n",
    "orig_gms = [f.detach() for f in orig_gms]\n",
    "len(orig_gms), len(orig_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.000071350 0.031473637\n",
      "10 0.000059971 0.041955624\n",
      "20 0.000056349 0.137909442\n",
      "30 0.000057158 0.202379227\n",
      "40 0.000056866 0.331571549\n",
      "50 0.000057623 0.760842562\n",
      "60 0.000059061 0.093008235\n",
      "70 0.000057771 0.031847019\n",
      "80 0.000058020 0.033376478\n",
      "90 0.000057771 0.031498905\n",
      "100 0.000057233 0.031791426\n",
      "110 0.000059117 0.031756070\n",
      "120 0.000056932 0.032664016\n",
      "130 0.000057763 0.038236737\n",
      "140 0.000056847 0.046444573\n",
      "150 0.000057292 0.078013174\n",
      "160 0.000057829 0.080911465\n",
      "170 0.000056318 0.072313741\n",
      "180 0.000059508 0.236562967\n",
      "190 0.000064147 0.264522910\n",
      "200 0.000061632 0.123133354\n",
      "210 0.000055893 0.064897180\n",
      "220 0.000058008 0.033878632\n",
      "230 0.000057916 0.037398115\n",
      "240 0.000060586 0.033362877\n",
      "250 0.000057084 0.035077214\n",
      "260 0.000057967 0.042399820\n",
      "270 0.000057764 0.035176422\n",
      "280 0.000057221 0.038449936\n",
      "290 0.000059794 0.043243349\n",
      "300 0.000057004 0.038780048\n",
      "310 0.000056468 0.036560073\n",
      "320 0.000057100 0.034269549\n",
      "330 0.000055947 0.043129440\n",
      "340 0.000057921 0.038498584\n",
      "350 0.000057975 0.039371870\n",
      "360 0.000058178 0.034226090\n",
      "370 0.000059026 0.033453375\n",
      "380 0.000057272 0.036855664\n",
      "390 0.000059137 0.041237541\n",
      "400 0.000057224 0.034746662\n",
      "410 0.000058673 0.046775959\n",
      "420 0.000058537 0.036605787\n",
      "430 0.000062712 0.096954167\n",
      "440 0.000062159 0.037621100\n",
      "450 0.000061653 0.034281570\n",
      "460 0.000060575 0.032082215\n",
      "470 0.000061118 0.032694764\n",
      "480 0.000061416 0.034117371\n",
      "490 0.000059744 0.033643924\n",
      "500 0.000058645 0.039577805\n",
      "510 0.000059529 0.042488225\n",
      "520 0.000058972 0.050060436\n",
      "530 0.000056575 0.045569174\n",
      "540 0.000059598 0.042965192\n",
      "550 0.000061022 0.034979340\n",
      "560 0.000061114 0.039630435\n",
      "570 0.000059861 0.036431082\n",
      "580 0.000060133 0.037751503\n",
      "590 0.000060437 0.037277669\n",
      "600 0.000061361 0.051282670\n",
      "610 0.000060696 0.042977381\n",
      "620 0.000060578 0.041888695\n",
      "630 0.000060329 0.061911643\n",
      "640 0.000057484 0.050282869\n",
      "650 0.000057966 0.042367786\n",
      "660 0.000060282 0.052900538\n",
      "670 0.000059893 0.032946348\n",
      "680 0.000059644 0.034238309\n",
      "690 0.000058722 0.041530684\n",
      "700 0.000059890 0.041097850\n",
      "710 0.000057977 0.034231827\n",
      "720 0.000060017 0.043474477\n",
      "730 0.000057985 0.035698101\n",
      "740 0.000058960 0.033699103\n",
      "750 0.000059506 0.036731977\n",
      "760 0.000060826 0.038386974\n",
      "770 0.000058723 0.040450841\n",
      "780 0.000060721 0.040354699\n",
      "790 0.000060002 0.053160504\n",
      "800 0.000058651 0.042149961\n",
      "810 0.000057275 0.052875571\n",
      "820 0.000057280 0.050134588\n",
      "830 0.000058813 0.033206102\n",
      "840 0.000055992 0.034936570\n",
      "850 0.000056596 0.038620204\n",
      "860 0.000054734 0.034858961\n",
      "870 0.000056223 0.037199467\n",
      "880 0.000055918 0.033547893\n",
      "890 0.000054937 0.041574150\n",
      "900 0.000054842 0.036169283\n",
      "910 0.000053767 0.036460724\n",
      "920 0.000053973 0.047357649\n",
      "930 0.000052778 0.033139188\n",
      "940 0.000053533 0.031598713\n",
      "950 0.000052888 0.031590495\n",
      "960 0.000052923 0.031988371\n",
      "970 0.000052976 0.031525768\n",
      "980 0.000053192 0.031479135\n",
      "990 0.000054061 0.032404967\n",
      "1000 0.000052177 0.031619430\n"
     ]
    }
   ],
   "source": [
    "steps = 1001\n",
    "\n",
    "vert = vertices.clone()\n",
    "\n",
    "for step in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    vertices_exp = vertices.expand(bs, -1, -1, -1)\n",
    "    vert = vertices_exp.clone()\n",
    "    vert += torch.randn_like(vert) * 0.0002\n",
    "    normals = calculate_normals(vert)\n",
    "    vert, colors = G(vert, normals)\n",
    "    normals  = calculate_normals(vert.detach())\n",
    "    \n",
    "    vrt_a, nrm_a = get_aligned(vert, normals)\n",
    "    renders = R(vrt_a, colors, grayscale=False)\n",
    "    \n",
    "    renders = normalize(renders)\n",
    "    \n",
    "    gen_features = model(renders)\n",
    "    style_loss = calc_style_loss(gen_features, orig_gms)\n",
    "    diff_loss = F.mse_loss(vert, vertices_exp, reduction='sum' )\n",
    "    loss = style_loss * 10000 + diff_loss * 0.1\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    p = 0.05\n",
    "    vert = vert.detach().mean(dim=0, keepdim=True)\n",
    "    vertices = (1-p) * vertices+ (p* vert.detach())\n",
    "    if step % 10 == 0:\n",
    "        print(step, f\"{style_loss.item():.9f}\", f\"{diff_loss.item():.9f}\")\n",
    "    if  step % 10 == 0: # step < 100 or \n",
    "        renders = R(vrt_a, colors, grayscale=False)\n",
    "        img = TR.ToPILImage()(renders[0])\n",
    "        img.save(f'./data/fit/5_{step}.png')\n",
    "img.save(f'./data/fit/5_{steps}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_0.3",
   "language": "python",
   "name": "pytorch3d_0.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
