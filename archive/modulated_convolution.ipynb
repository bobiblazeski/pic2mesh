{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 8, 8])\n",
      "torch.Size([1, 3, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fused_leaky_relu(x, bias, negative_slope=0.2, scale=2 ** 0.5):\n",
    "    return scale * F.leaky_relu(x + bias.view((1, -1)+(1,)*(len(x.shape)-2)), \n",
    "                                negative_slope=negative_slope)\n",
    "\n",
    "def upfirdn2d_native(\n",
    "    x, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n",
    "):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    _, in_h, in_w, minor = x.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    out = x.view(-1, in_h, 1, in_w, 1, minor)\n",
    "    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n",
    "    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n",
    "\n",
    "    out = F.pad(\n",
    "        out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n",
    "    )\n",
    "    out = out[\n",
    "        :,\n",
    "        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n",
    "        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n",
    "        :,\n",
    "    ]\n",
    "\n",
    "    out = out.permute(0, 3, 1, 2)\n",
    "    out = out.reshape(\n",
    "        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n",
    "    )\n",
    "    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n",
    "    out = F.conv2d(out, w)\n",
    "    out = out.reshape(\n",
    "        -1,\n",
    "        minor,\n",
    "        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n",
    "        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n",
    "    )\n",
    "    # out = out.permute(0, 2, 3, 1)\n",
    "    return out[:, :, ::down_y, ::down_x]\n",
    "\n",
    "\n",
    "def upfirdn2d(x, kernel, up=1, down=1, pad=(0, 0)):    \n",
    "    return upfirdn2d_native(x, kernel, up, up, down, down, \n",
    "                           pad[0], pad[1], pad[0], pad[1])    \n",
    "\n",
    "class EqualizedLinear(nn.Module):\n",
    "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, bias=True, bias_init=0., activation=None,\n",
    "                 gain=1., use_wscale=True, lrmul=1.):\n",
    "        super(EqualizedLinear, self).__init__()\n",
    "\n",
    "        # Equalized learning rate and custom learning rate multiplier.\n",
    "        he_std = gain * in_dim ** (-0.5)  # He init\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "\n",
    "        self.weight = torch.nn.Parameter(torch.randn(\n",
    "            out_dim, in_dim) * init_std, requires_grad=True)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(\n",
    "                out_dim).fill_(bias_init), requires_grad=True)\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation == 'lrelu':  # act='lrelu'\n",
    "            out = F.linear(x, self.weight * self.w_mul)\n",
    "            out = fused_leaky_relu(out, self.bias * self.b_mul)\n",
    "        else:\n",
    "            out = F.linear(x, self.weight * self.w_mul,\n",
    "                           bias=self.bias * self.b_mul)\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "print(upfirdn2d(torch.randn(1,3,10,10).cuda(),torch.randn(3,3).cuda()).shape)\n",
    "print(upfirdn2d(torch.randn(1,3,10,10), torch.randn(3,3)).shape)\n",
    "\n",
    "el = EqualizedLinear(64, 32, activation='lrelu')\n",
    "el(torch.rand(3, 64)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_kernel(k):\n",
    "    k = torch.tensor(k, dtype=torch.float32)\n",
    "\n",
    "    if k.ndim == 1:\n",
    "        k = k[None, :] * k[:, None]\n",
    "\n",
    "    k /= k.sum()\n",
    "\n",
    "    return k\n",
    "\n",
    "class Blur(nn.Module):\n",
    "    def __init__(self, kernel, pad, upsample_factor=1):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel = make_kernel(kernel)\n",
    "\n",
    "        if upsample_factor > 1:\n",
    "            kernel = kernel * (upsample_factor ** 2)\n",
    "\n",
    "        self.register_buffer('kernel', kernel)\n",
    "\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = upfirdn2d(x, self.kernel, pad=self.pad)\n",
    "\n",
    "        return out\n",
    "    \n",
    "blur = Blur((3, 3), (0, 0))\n",
    "blur(torch.rand((2, 4, 3, 3))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 128, 8, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EqualizedModConv2d(nn.Module):\n",
    "    def __init__(self, dlatent_size, in_channel, out_channel, kernel,\n",
    "                 up=False, down=False, demodulate=True, resample_kernel=None,\n",
    "                 gain=1., use_wscale=True, lrmul=1.):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(EqualizedModConv2d, self).__init__()\n",
    "\n",
    "        assert not (up and down)\n",
    "        assert kernel >= 1 and kernel % 2 == 1\n",
    "\n",
    "        if resample_kernel is None:\n",
    "            resample_kernel = [1, 3, 3, 1]\n",
    "\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.demodulate = demodulate\n",
    "        self.kernel = kernel\n",
    "\n",
    "        if up:\n",
    "            factor = 2\n",
    "            p = (len(resample_kernel) - factor) - (kernel - 1)\n",
    "            self.blur = Blur(resample_kernel, pad=(\n",
    "                (p + 1) // 2 + factor - 1, p // 2 + 1), upsample_factor=factor)\n",
    "\n",
    "        if down:\n",
    "            factor = 2\n",
    "            p = (len(resample_kernel) - factor) + (kernel - 1)\n",
    "            self.blur = Blur(resample_kernel, pad=((p + 1) // 2, p // 2))\n",
    "\n",
    "        self.mod = EqualizedLinear(\n",
    "            in_dim=dlatent_size, out_dim=in_channel, bias_init=1.)\n",
    "\n",
    "        he_std = gain * (in_channel * kernel ** 2) ** (-0.5)  # He init\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.randn(1, out_channel, in_channel, kernel, kernel) * init_std, requires_grad=True)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch, in_channel, height, width = x.shape\n",
    "\n",
    "        # Modulate\n",
    "        s = self.mod(y).view(batch, 1, in_channel, 1, 1)\n",
    "        ww = self.w_mul * self.weight * s\n",
    "\n",
    "        # Demodulate\n",
    "        if self.demodulate:\n",
    "            # [BO] Scaling factor.\n",
    "            d = torch.rsqrt(ww.pow(2).sum([2, 3, 4]) + 1e-8)\n",
    "            # [BOIkk] Scale output feature maps.\n",
    "            ww *= d.view(batch, self.out_channel, 1, 1, 1)\n",
    "\n",
    "        weight = ww.view(batch * self.out_channel,\n",
    "                         in_channel, self.kernel, self.kernel)\n",
    "\n",
    "        if self.up:\n",
    "            x = x.view(1, batch * in_channel, height, width)\n",
    "            weight = weight.view(batch, self.out_channel,\n",
    "                                 in_channel, self.kernel, self.kernel)\n",
    "            weight = weight.transpose(1, 2).reshape(batch * in_channel, self.out_channel,\n",
    "                                                    self.kernel, self.kernel)\n",
    "            out = F.conv_transpose2d(\n",
    "                x, weight, padding=0, stride=2, groups=batch)\n",
    "            _, _, height, width = out.shape\n",
    "            out = out.view(batch, self.out_channel, height, width)\n",
    "            out = self.blur(out)\n",
    "        elif self.down:\n",
    "            x = self.blur(x)\n",
    "            _, _, height, width = x.shape\n",
    "            x = x.view(1, batch * in_channel, height, width)\n",
    "            out = F.conv2d(x, weight, padding=0, stride=2, groups=batch)\n",
    "            _, _, height, width = out.shape\n",
    "            out = out.view(batch, self.out_channel, height, width)\n",
    "        else:\n",
    "            x = x.view(1, batch * in_channel, height, width)\n",
    "            out = F.conv2d(x, weight, padding=self.kernel // 2, groups=batch)\n",
    "            _, _, height, width = out.shape\n",
    "            out = out.view(batch, self.out_channel, height, width)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'{self.__class__.__name__}({self.in_channel}, {self.out_channel}, {self.kernel_size}, '\n",
    "            f'upsample={self.up}, downsample={self.down})'\n",
    "        )\n",
    "\n",
    "dlatent_size = 256\n",
    "in_channel = 64\n",
    "out_channel = 128\n",
    "kernel = 3\n",
    "modconv2d = EqualizedModConv2d(dlatent_size, in_channel, out_channel, kernel,\n",
    "                     up=False, down=False, demodulate=True, resample_kernel=None,\n",
    "                     gain=1., use_wscale=True, lrmul=1.)    \n",
    "\n",
    "bs = 7\n",
    "modconv2d(torch.rand((bs, in_channel, 8, 8)),\n",
    "          torch.rand((bs, dlatent_size))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_0.3",
   "language": "python",
   "name": "pytorch3d_0.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
