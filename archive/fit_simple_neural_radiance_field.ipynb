{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %matplotlib notebook\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.transforms import so3_exponential_map\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras,\n",
    "    NDCGridRaysampler,\n",
    "    MonteCarloRaysampler,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    ImplicitRenderer,\n",
    "    RayBundle,\n",
    "    ray_bundle_to_ray_points,\n",
    ")\n",
    "\n",
    "# add path for demo utils functions\n",
    "sys.path.append(os.path.abspath(''))\n",
    "from utils.plot_image_grid import image_grid\n",
    "from utils.generate_cow_renders import generate_cow_renders\n",
    "\n",
    "# obtain the utilized device\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cameras, target_images, target_silhouttes = generate_cow_renders(\n",
    "num_views=40, azimuth_range=180,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40 images/silhouettes/cameras.\n"
     ]
    }
   ],
   "source": [
    "print(f'Generated {len(target_images)} images/silhouettes/cameras.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_size = target_images.shape[1] * 2\n",
    "\n",
    "volume_extent_world = 3.0\n",
    "\n",
    "raysampler_grid = NDCGridRaysampler(\n",
    "    image_height=render_size,\n",
    "    image_width=render_size,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "raysampler_mc = MonteCarloRaysampler(\n",
    "    min_x = -1.0,\n",
    "    max_x =  1.0,\n",
    "    min_y = -1.0,\n",
    "    max_y =  1.0,\n",
    "    n_rays_per_image=750,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "\n",
    "renderer_grid = ImplicitRenderer(\n",
    "    raysampler=raysampler_grid, raymarcher=raymarcher,\n",
    ")\n",
    "\n",
    "renderer_mc = ImplicitRenderer(\n",
    "    raysampler=raysampler_mc, raymarcher=raymarcher,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicEmbedding(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, omega0=0.1):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            'frequencies',\n",
    "            omega0 * (2.0 ** torch.arange(n_harmonic_functions)),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        embed = (x[..., None] * self.frequencies).view(\n",
    "            *x.shape[:-1], -1)\n",
    "        return torch.cat((embed.sin(), embed.cos()), dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he = HarmonicEmbedding(n_harmonic_functions=2)\n",
    "he(torch.rand(1, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0862, 0.1717, 0.0335, 0.0669, 0.9963, 0.9852, 0.9994, 0.9978]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he(torch.rand(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he.frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralRadianceField(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, \n",
    "                 n_hidden_neurons=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_harmonic_functions: The number of harmonic functions\n",
    "                used to form the harmonic embedding of each point\n",
    "            n_hidden_neurons: The number of hidden units in the \n",
    "               fully connected layers of the MLPs of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        # The harmonic embedding layer converts input 3D coordinates\n",
    "        # to a representation that is more suitable for \n",
    "        # processing with a deep neural network\n",
    "        self.harmonic_embedding = HarmonicEmbedding(n_harmonic_functions)\n",
    "        \n",
    "        # The dimension of the harmonic embedding.\n",
    "        embedding_dim = n_harmonic_functions * 2 * 3\n",
    "        \n",
    "        # self.mlp is a simple 2-layer multi-layer perceptron\n",
    "        # which converts the input per-point harmonic embeddings\n",
    "        # to a latent representation.\n",
    "        # Not that we use Softplus activations instead of RELU\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "        )\n",
    "        \n",
    "        # Given features predicted by self.mlp, self.color_layer\n",
    "        # is responsible for predicting a 3-D per-point vector\n",
    "        # that represents the RGB color of the point.\n",
    "        self.color_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons + embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, 3),\n",
    "            torch.nn.Sigmoid(),\n",
    "            # To ensure that the colors correctly range between [0-1],\n",
    "            # the layer is terminated with a sigmoid layer.\n",
    "        )\n",
    "        \n",
    "        # The density layer converts the features of self.mlp\n",
    "        # to a 1D density value representing the raw opacity \n",
    "        # of each point.\n",
    "        self.density_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons, 1),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            # Softplus activation ensures that the raw opacity\n",
    "            # is a non-negative number.\n",
    "        )\n",
    "        \n",
    "        # We set the bias of the density layer to -1.5\n",
    "        # in order to initialize the opacities of the \n",
    "        # ray points to values close to 0.\n",
    "        # This is a crucial detail for ensuring convergence\n",
    "        # of the model\n",
    "        self.density_layer[0].bias,data[0] = -1.5\n",
    "    \n",
    "    def __get_densities(self, features):\n",
    "        \"\"\"\n",
    "        This function takes `features` predicted by `self.mlp`\n",
    "        and converts them `raw_densities` with `self.density_layer`.\n",
    "        `raw_densities` are kater mapped to [0-1] range with\n",
    "        1 - inverse exponential of `raw_densities`.\n",
    "        \"\"\"\n",
    "        raw_densities =  self.density_layer(features)\n",
    "        return 1 - (-raw_densities).exp()\n",
    "        \n",
    "    def __get_colors(self, features, rays_directions):\n",
    "        \"\"\"\n",
    "        This function stakes per-point `features` predicted by `self.mlp`\n",
    "        and elevates the color model in order to attach to each\n",
    "        point a 3D vector of its RGB color.\n",
    "        \n",
    "        In order to represent viewpoint dependent effects,\n",
    "        before evaluating `self.color_layer`, `NeuralRadianceField`\n",
    "        concatenas the features of the harmonic embedding\n",
    "        of ray directions which are per-point directions \n",
    "        of point  rays expressed as 3D l2-normalized vectors\n",
    "        in world coordinates.\n",
    "        \"\"\"\n",
    "        spatial_size = features.shape[:-1]\n",
    "        \n",
    "        # Normalize the ray directions to unit l2 norm\n",
    "        rays_directions_normed = torch.nn.functional.normalize(\n",
    "            rays_direcions, dim=-1\n",
    "        )\n",
    "        \n",
    "        # Obtain the harmonic embedding of the normalized ray directions.\n",
    "        rays_embedding = self.harmonic_embedding(\n",
    "            rays_directions_normed\n",
    "        )\n",
    "        \n",
    "        # Expand the rays directions tensor so that its spatial size\n",
    "        # is equal to the size of features.\n",
    "        rays_embedding_expand = rays_embedding[..., None, :].expand(\n",
    "            *spatial_size, rays_embedding.shape[-1]\n",
    "        )\n",
    "        \n",
    "        # Concatenate ray direction embeddings with\n",
    "        # features and evaluate the color model.\n",
    "        color_layer_input = torch.cat(\n",
    "            (features, rays_embedding_expand),\n",
    "            dim=-1\n",
    "        )\n",
    "        return self.color_layer(color_layer_input)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        ray_bundle: RayBundle,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The forward functions accepts the parametrization of\n",
    "        3D points sampled along projection rays. The forward\n",
    "        pass is responsible for attaching a 3D vector\n",
    "        and a 1D scalar representing the point's\n",
    "        RGB color and opacity respectivily.\n",
    "        \n",
    "        Args:\n",
    "            ray_bundle: A RayBundle object containing the fallowing variables:\n",
    "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
    "                    origins of the sampling rays in world coords.\n",
    "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
    "                    containing the direction vectoirs of sampling rays in world coords\n",
    "                length: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
    "                    containing the lengths at which rays are sampled.\n",
    "                    \n",
    "        Returns:\n",
    "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the opacity of each ray point\n",
    "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the color of each ray point\n",
    "        \"\"\"\n",
    "        # We first convert the ray parametrizations to world\n",
    "        # coordinates with `ray_bundle_to_ray_points`.\n",
    "        rays_points_world = ray_bundle_to_ray_points(ray_bundle)\n",
    "        # rays_points_world.shape = [minibatch x ... x 3]\n",
    "        \n",
    "        # For each 3D world coordinate, we obtain its harmonic embedding\n",
    "        embeds = self.harmonic_embedding(\n",
    "            rays_points_world\n",
    "        )\n",
    "        \n",
    "        features = self.mlp(embeds)\n",
    "        \n",
    "        rays_densities =  self._densities(features)        \n",
    "        \n",
    "        rays_colors = self._get_colors(features, ray_bundle.directions)\n",
    "        \n",
    "        return rays_densities, rays_colors\n",
    "    \n",
    "    def batched_forward(\n",
    "        self, \n",
    "        ray_bundle: RayBundle,\n",
    "        n_batches: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        n_pts_per_ray = ray_bundle.lengths.shape[-1]\n",
    "        spatial_size = [*ray_bundle.origins.shape[:-1], n_pts_per_ray]\n",
    "        \n",
    "        # Split the ray to `n_batches` batches\n",
    "        tot_samples = ray_bundle.origins.shape[:-1].numel()\n",
    "        batches = torch.chunk(torch.arange(tot_samples), n_batches)\n",
    "        \n",
    "        \n",
    "        # For each batch execute forward pass\n",
    "        batch_outputs = [\n",
    "            self.forward(\n",
    "                RayBundle(\n",
    "                    origins=ray_bundle.origins.view(-1, 3)[batch_idx],\n",
    "                    directions=ray_bundle.directions.view(-1, 3)[batch_idx],\n",
    "                    lengths=ray_bundle.lengths.view(-1, n_pts_per_ray)[batch_idx],\n",
    "                    xys=None\n",
    "                ) for batch_idx in batches\n",
    "            )            \n",
    "        ]\n",
    "        \n",
    "        # Concatenate the per batch rays_densities and rays colors\n",
    "        # and reshape according to the sizes of the inputs\n",
    "        rays_densities, rays_colors = [\n",
    "            torch.cat(\n",
    "                [batch_output[output_i] for batch_output in batch_outputs], dim=0\n",
    "            ).view(*spatial_size, -1) for output_i in (0, 1)\n",
    "        ]\n",
    "        return rays_densities, rays_colors        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber(x, y, scaling=0.1):\n",
    "    \"\"\"\n",
    "    A helper function for evaluating the smooth L1 (huber) loss\n",
    "    between the renderer silhouettes and colors.\n",
    "    \"\"\"\n",
    "    diff_sq = (x - y) ** 2\n",
    "    loss = ((1 + diff_sq / (scaling**2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n",
    "    return loss\n",
    "\n",
    "def sample_images_at_mc_locs(target_images, sampled_rays_xy):\n",
    "    \"\"\"\n",
    "    Given a set of Monte Carlo pixel locations `sampled_rays_xy`,\n",
    "    this methods samples the tensor `target_images` at the\n",
    "    respective  2D locations.\n",
    "    \n",
    "    This function is used in order to extract the colors from\n",
    "    ground truth images that correspond to the colors\n",
    "    rendered using `MonteCarloRaysampler`\n",
    "    \"\"\"\n",
    "    ba = target_images.shape[0]\n",
    "    dim = target_images.shape[-1]\n",
    "    spatial_size = sampled_rays_xy.shape[1:-1]\n",
    "    \n",
    "    images_sampled = torch.nn.functional.grid_sample(\n",
    "        target_images.permute(0, 3, 1, 2),\n",
    "        -sampled_rays_xy.view(ba, -1, 1, 2),  # note the sign inversion\n",
    "        align_corners=True\n",
    "    )\n",
    "    return images_sampled.permute(0, 2, 3, 1).view(\n",
    "        ba, *spatial_size, dim\n",
    "    )\n",
    "\n",
    "def show_full_render(\n",
    "    neural_radiance_field, camera,\n",
    "    target_image, target_silhouette,\n",
    "    loss_history_color, loss_history_sil,\n",
    "):\n",
    "    # Prevent gradient caching\n",
    "    with torch.no_grad():\n",
    "        # render using grid renderer and the batched \n",
    "        # forward function of neural radiance field\n",
    "        rendered_image_silhouette, _ = renderer_grid(\n",
    "            cameras=camera,\n",
    "            volumetric_function=neural_radiance_field.batched_forward\n",
    "        )\n",
    "        # Split the rendering result to a silhouette render\n",
    "        # and the image render\n",
    "        rendered_image, rendered_silhouette, _ = (\n",
    "            rendered_image_silhouette[0].split([3, 1], dim=-1)\n",
    "        )\n",
    "        \n",
    "     # Generate plots.\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    ax = ax.ravel()\n",
    "    clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()\n",
    "    ax[0].plot(list(range(len(loss_history_color))), loss_history_color, linewidth=1)\n",
    "    ax[1].imshow(clamp_and_detach(rendered_image))\n",
    "    ax[2].imshow(clamp_and_detach(rendered_silhouette[..., 0]))\n",
    "    ax[3].plot(list(range(len(loss_history_sil))), loss_history_sil, linewidth=1)\n",
    "    ax[4].imshow(clamp_and_detach(target_image))\n",
    "    ax[5].imshow(clamp_and_detach(target_silhouette))\n",
    "    for ax_, title_ in zip(\n",
    "        ax,\n",
    "        (\n",
    "            \"loss color\", \"rendered image\", \"rendered silhouette\",\n",
    "            \"loss silhouette\", \"target image\",  \"target silhouette\",\n",
    "        )\n",
    "    ):\n",
    "        if not title_.startswith('loss'):\n",
    "            ax_.grid(\"off\")\n",
    "            ax_.axis(\"off\")\n",
    "        ax_.set_title(title_)\n",
    "    fig.canvas.draw(); fig.show()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    return fig\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_silhouettes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0a5d91d4d279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_cameras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_cameras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtarget_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtarget_silhouettes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_silhouettes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Set the seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_silhouettes' is not defined"
     ]
    }
   ],
   "source": [
    "# First move all relevant variables to the correct device.\n",
    "renderer_grid = renderer_grid.to(device)\n",
    "renderer_mc = renderer_mc.to(device)\n",
    "target_cameras = target_cameras.to(device)\n",
    "target_images = target_images.to(device)\n",
    "target_silhouettes = target_silhouettes.to(device)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Instantiate the radiance field model.\n",
    "neural_radiance_field = NeuralRadianceField().to(device)\n",
    "\n",
    "# Instantiate the Adam optimizer. We set its master learning rate to 1e-3.\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=lr)\n",
    "\n",
    "# We sample 6 random cameras in a minibatch. Each camera\n",
    "# emits raysampler_mc.n_pts_per_image rays.\n",
    "batch_size = 6\n",
    "\n",
    "# 3000 iterations take ~20 min on a Tesla M40 and lead to\n",
    "# reasonably sharp results. However, for the best possible\n",
    "# results, we recommend setting n_iter=20000.\n",
    "n_iter = 3000\n",
    "\n",
    "# Init the loss history buffers.\n",
    "loss_history_color, loss_history_sil = [], []\n",
    "\n",
    "# The main optimization loop.\n",
    "for iteration in range(n_iter):      \n",
    "    # In case we reached the last 75% of iterations,\n",
    "    # decrease the learning rate of the optimizer 10-fold.\n",
    "    if iteration == round(n_iter * 0.75):\n",
    "        print('Decreasing LR 10-fold ...')\n",
    "        optimizer = torch.optim.Adam(\n",
    "            neural_radiance_field.parameters(), lr=lr * 0.1\n",
    "        )\n",
    "    \n",
    "    # Zero the optimizer gradient.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample random batch indices.\n",
    "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
    "    \n",
    "    # Sample the minibatch of cameras.\n",
    "    batch_cameras = FoVPerspectiveCameras(\n",
    "        R = target_cameras.R[batch_idx], \n",
    "        T = target_cameras.T[batch_idx], \n",
    "        znear = target_cameras.znear[batch_idx],\n",
    "        zfar = target_cameras.zfar[batch_idx],\n",
    "        aspect_ratio = target_cameras.aspect_ratio[batch_idx],\n",
    "        fov = target_cameras.fov[batch_idx],\n",
    "        device = device,\n",
    "    )\n",
    "    \n",
    "    # Evaluate the nerf model.\n",
    "    rendered_images_silhouettes, sampled_rays = renderer_mc(\n",
    "        cameras=batch_cameras, \n",
    "        volumetric_function=neural_radiance_field\n",
    "    )\n",
    "    rendered_images, rendered_silhouettes = (\n",
    "        rendered_images_silhouettes.split([3, 1], dim=-1)\n",
    "    )\n",
    "    \n",
    "    # Compute the silhoutte error as the mean huber\n",
    "    # loss between the predicted masks and the\n",
    "    # sampled target silhouettes.\n",
    "    silhouettes_at_rays = sample_images_at_mc_locs(\n",
    "        target_silhouettes[batch_idx, ..., None], \n",
    "        sampled_rays.xys\n",
    "    )\n",
    "    sil_err = huber(\n",
    "        rendered_silhouettes, \n",
    "        silhouettes_at_rays,\n",
    "    ).abs().mean()\n",
    "\n",
    "    # Compute the color error as the mean huber\n",
    "    # loss between the rendered colors and the\n",
    "    # sampled target images.\n",
    "    colors_at_rays = sample_images_at_mc_locs(\n",
    "        target_images[batch_idx], \n",
    "        sampled_rays.xys\n",
    "    )\n",
    "    color_err = huber(\n",
    "        rendered_images, \n",
    "        colors_at_rays,\n",
    "    ).abs().mean()\n",
    "    \n",
    "    # The optimization loss is a simple\n",
    "    # sum of the color and silhouette errors.\n",
    "    loss = color_err + sil_err\n",
    "    \n",
    "    # Log the loss history.\n",
    "    loss_history_color.append(float(color_err))\n",
    "    loss_history_sil.append(float(sil_err))\n",
    "    \n",
    "    # Every 10 iterations, print the current values of the losses.\n",
    "    if iteration % 10 == 0:\n",
    "        print(\n",
    "            f'Iteration {iteration:05d}:'\n",
    "            + f' loss color = {float(color_err):1.2e}'\n",
    "            + f' loss silhouette = {float(sil_err):1.2e}'\n",
    "        )\n",
    "    \n",
    "    # Take the optimization step.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Visualize the full renders every 100 iterations.\n",
    "    if iteration % 100 == 0:\n",
    "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
    "        show_full_render(\n",
    "            neural_radiance_field,\n",
    "            FoVPerspectiveCameras(\n",
    "                R = target_cameras.R[show_idx], \n",
    "                T = target_cameras.T[show_idx], \n",
    "                znear = target_cameras.znear[show_idx],\n",
    "                zfar = target_cameras.zfar[show_idx],\n",
    "                aspect_ratio = target_cameras.aspect_ratio[show_idx],\n",
    "                fov = target_cameras.fov[show_idx],\n",
    "                device = device,\n",
    "            ), \n",
    "            target_images[show_idx][0],\n",
    "            target_silhouettes[show_idx][0],\n",
    "            loss_history_color,\n",
    "            loss_history_sil,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_0.3",
   "language": "python",
   "name": "pytorch3d_0.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
